{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## frathom\n",
    "## Homework 4 Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JEWuwxHsyZyP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as tf\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "import helper\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset\n",
    "import glob\n",
    "import librosa\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pWcUTtluynr8",
    "outputId": "a3715539-39d3-4763-cc82-80f958e39ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Tz8Ee1xsEcu-"
   },
   "outputs": [],
   "source": [
    "folder_path='/content/gdrive/My Drive/HW4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8BTksmDhIO7x",
    "outputId": "9c330481-3935-445e-8fe5-9a579d4f3c94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hw4_te7.pkl  hw4_tes.pkl  hw4_tr7.pkl  hw4_trs.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls '/content/gdrive/My Drive/HW4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dc4xj9OQISj4"
   },
   "outputs": [],
   "source": [
    "folder_list=os.listdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QVTk9-WNKMhT",
    "outputId": "47a447a6-297c-464b-9676-33e65fe6414f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hw4_trs.pkl', 'hw4_tr7.pkl', 'hw4_te7.pkl', 'hw4_tes.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "38CJZVTuKO_A"
   },
   "outputs": [],
   "source": [
    "for i in folder_list:\n",
    "  if(i=='hw4_trs.pkl'):\n",
    "    trs_file= open(folder_path+i,'rb')\n",
    "    train=pickle.load(trs_file)\n",
    "    trs_file.close()\n",
    "      \n",
    "  if(i=='hw4_tes.pkl'):\n",
    "    tes_file= open(folder_path+i,'rb')\n",
    "    test=pickle.load(tes_file)\n",
    "    tes_file.close()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kya7r1GwQNrt"
   },
   "outputs": [],
   "source": [
    "train_s=[]\n",
    "for i in train:\n",
    "  sig_train=librosa.stft(i,n_fft=1024,hop_length=512)\n",
    "  train_s.append(np.abs(sig_train.T))\n",
    "train_s=np.array(train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pBK_qx9PvNgW"
   },
   "outputs": [],
   "source": [
    "test_s=[]\n",
    "for i in test:\n",
    "  sig_test=librosa.stft(i,n_fft=1024,hop_length=512)\n",
    "  test_s.append(np.abs(sig_test.T))\n",
    "test_s=np.array(test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wn38MQK2wLl0",
    "outputId": "3ea2c63a-ce44-451a-9505-5030a7d91f84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 32, 513)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51E-pWYkKq2N",
    "outputId": "96dd1831-2d2f-4185-f003-9a22ac7d34dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 45, 513)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ri58vhUWVWgo"
   },
   "outputs": [],
   "source": [
    "def comb(s,e):\n",
    "  return list(combinations(range(s,e),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1uRNWYSAUqLZ"
   },
   "outputs": [],
   "source": [
    "def repeat(to_pa,data):\n",
    "  #total_pairs=np.array(to_pa)\n",
    "  if(data=='train'):\n",
    "    s1=train_s[total_pairs[:,0]]\n",
    "    s2=train_s[total_pairs[:,1]]\n",
    "    labels=np.tile(np.concatenate((np.ones((20,1)), np.zeros((20,1))), axis = 0),(50,1))\n",
    "\n",
    "  if(data=='test'):\n",
    "    s1=test_s[total_pairs[:,0]]\n",
    "    s2=test_s[total_pairs[:,1]]\n",
    "    labels=np.tile(np.concatenate((np.ones((20,1)), np.zeros((20,1))), axis = 0),(20,1))\n",
    "\n",
    "  return s1,s2,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9ZgO2aieKsvy"
   },
   "outputs": [],
   "source": [
    "total=train_s.shape[0]\n",
    "to_pa=[]\n",
    "en=0\n",
    "for i in range(50):\n",
    "  st=en\n",
    "  en=(i+1)*10\n",
    "  co=comb(st,en)\n",
    "  positive=random.sample(co,20)\n",
    "  current=list(np.random.choice(range(st,en),20))\n",
    "  others=random.sample(list(set(range(total))-set(range(st,en))),20)\n",
    "  negative=list(zip(current,others))\n",
    "  #to_pa=to_pa.extend(positive)\n",
    "  #to_pa=to_pa.extend(negative)\n",
    "  to_pa=to_pa+positive+negative\n",
    "\n",
    "total_pairs=np.array(to_pa)\n",
    "s1_train,s2_train,train_labels=repeat(total_pairs,'train')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKXKrexpVXZr",
    "outputId": "375fa38a-fcd1-4d2b-edfb-f89fde9ce57b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels=np.reshape(train_labels,(train_labels.shape[0],))\n",
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Xcu79TC5yC9f"
   },
   "outputs": [],
   "source": [
    "total=test_s.shape[0]\n",
    "to_pa=[]\n",
    "en=0\n",
    "for i in range(20):\n",
    "  st=en\n",
    "  en=(i+1)*10\n",
    "  co=comb(st,en)\n",
    "  positive=random.sample(co,20)\n",
    "  current=list(np.random.choice(range(st,en),20))\n",
    "  others=random.sample(list(set(range(total))-set(range(st,en))),20)\n",
    "  negative=list(zip(current,others))\n",
    "  #to_pa=to_pa.extend(positive)\n",
    "  #to_pa=to_pa.extend(negative)\n",
    "  to_pa=to_pa+positive+negative\n",
    "\n",
    "total_pairs=np.array(to_pa)\n",
    "s1_test,s2_test,test_labels=repeat(total_pairs,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9sZ-8QwJXCMG",
    "outputId": "c635d46c-c3ba-46ae-cca4-ced42ffc8c8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels=np.reshape(test_labels,(test_labels.shape[0],))\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "id": "YYbm5QuOZz7X"
   },
   "outputs": [],
   "source": [
    "class SN(nn.Module):\n",
    "  def __init__(self,inp_size,hid_size,num_lay,out_size):\n",
    "    super(SN,self).__init__()\n",
    "    self.inp_size=inp_size\n",
    "    self.hid_size=hid_size\n",
    "    self.num_lay=num_lay\n",
    "    self.gru=nn.GRU(inp_size,hid_size,num_lay,batch_first=True,dropout=0.4)\n",
    "    self.lay1=nn.Linear(hid_size,hid_size)\n",
    "    self.lay2=nn.Linear(hid_size,out_size)\n",
    "    self.sig=nn.Sigmoid()\n",
    "    self.tanh=nn.Tanh()\n",
    "  \n",
    "  def common_forward(self,inp):\n",
    "    out,hid=self.gru(inp)\n",
    "    o=out[:,-1,:].squeeze()\n",
    "    tanh_out=self.tanh(self.lay2(self.lay1(o)))\n",
    "    return tanh_out\n",
    "\n",
    "  def forward(self,inp1,inp2):\n",
    "    o1=self.common_forward(inp1).unsqueeze(1)\n",
    "    o2=self.common_forward(inp2).unsqueeze(2)\n",
    "    out=torch.bmm(o1,o2).squeeze()\n",
    "    return self.sig(out)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "id": "ccZPPGrrxxNi"
   },
   "outputs": [],
   "source": [
    "model = SN( 513,16,2,150).cuda()\n",
    "loss = nn.BCELoss()\n",
    "optimize = torch.optim.Adam(model.parameters(),lr =0.0002)\n",
    "batch_num=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bPyOgN1lxx3l",
    "outputId": "a2d0f3fb-6ab8-450c-d051-1899cadd0d50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 is done and the loss is 2.1117162704467773\n",
      "epoch 1 is done and the loss is 1.8169549703598022\n",
      "epoch 2 is done and the loss is 1.6007604598999023\n",
      "epoch 3 is done and the loss is 1.5060206651687622\n",
      "epoch 4 is done and the loss is 1.4025521278381348\n",
      "epoch 5 is done and the loss is 1.2889007329940796\n",
      "epoch 6 is done and the loss is 1.1454260349273682\n",
      "epoch 7 is done and the loss is 1.1219291687011719\n",
      "epoch 8 is done and the loss is 1.1137303113937378\n",
      "epoch 9 is done and the loss is 1.0254050493240356\n",
      "epoch 10 is done and the loss is 0.995309591293335\n",
      "epoch 11 is done and the loss is 0.9139983654022217\n",
      "epoch 12 is done and the loss is 0.8536626100540161\n",
      "epoch 13 is done and the loss is 0.8031637668609619\n",
      "epoch 14 is done and the loss is 0.8143818378448486\n",
      "epoch 15 is done and the loss is 0.7893164753913879\n",
      "epoch 16 is done and the loss is 0.7568760514259338\n",
      "epoch 17 is done and the loss is 0.756738007068634\n",
      "epoch 18 is done and the loss is 0.7158694863319397\n",
      "epoch 19 is done and the loss is 0.7171359658241272\n",
      "epoch 20 is done and the loss is 0.7150517106056213\n",
      "epoch 21 is done and the loss is 0.6774336099624634\n",
      "epoch 22 is done and the loss is 0.7002310156822205\n",
      "epoch 23 is done and the loss is 0.6798518300056458\n",
      "epoch 24 is done and the loss is 0.6754276156425476\n",
      "epoch 25 is done and the loss is 0.6705250144004822\n",
      "epoch 26 is done and the loss is 0.6950329542160034\n",
      "epoch 27 is done and the loss is 0.6013184189796448\n",
      "epoch 28 is done and the loss is 0.6259329319000244\n",
      "epoch 29 is done and the loss is 0.6442190408706665\n",
      "epoch 30 is done and the loss is 0.60639488697052\n",
      "epoch 31 is done and the loss is 0.6018474698066711\n",
      "epoch 32 is done and the loss is 0.6261326670646667\n",
      "epoch 33 is done and the loss is 0.6075706481933594\n",
      "epoch 34 is done and the loss is 0.5761551856994629\n",
      "epoch 35 is done and the loss is 0.5529175996780396\n",
      "epoch 36 is done and the loss is 0.5865699648857117\n",
      "epoch 37 is done and the loss is 0.5411407947540283\n",
      "epoch 38 is done and the loss is 0.5487655401229858\n",
      "epoch 39 is done and the loss is 0.5238470435142517\n",
      "epoch 40 is done and the loss is 0.5650198459625244\n",
      "epoch 41 is done and the loss is 0.5188440680503845\n",
      "epoch 42 is done and the loss is 0.5489552021026611\n",
      "epoch 43 is done and the loss is 0.5057762265205383\n",
      "epoch 44 is done and the loss is 0.5026658177375793\n",
      "epoch 45 is done and the loss is 0.5148059129714966\n",
      "epoch 46 is done and the loss is 0.4606698751449585\n",
      "epoch 47 is done and the loss is 0.5148983597755432\n",
      "epoch 48 is done and the loss is 0.5017645359039307\n",
      "epoch 49 is done and the loss is 0.5070934891700745\n",
      "epoch 50 is done and the loss is 0.49392351508140564\n",
      "epoch 51 is done and the loss is 0.47392287850379944\n",
      "epoch 52 is done and the loss is 0.47625577449798584\n",
      "epoch 53 is done and the loss is 0.46897923946380615\n",
      "epoch 54 is done and the loss is 0.46182212233543396\n",
      "epoch 55 is done and the loss is 0.46905040740966797\n",
      "epoch 56 is done and the loss is 0.4400702118873596\n",
      "epoch 57 is done and the loss is 0.4050651490688324\n",
      "epoch 58 is done and the loss is 0.4835781455039978\n",
      "epoch 59 is done and the loss is 0.4289563000202179\n",
      "epoch 60 is done and the loss is 0.418180376291275\n",
      "epoch 61 is done and the loss is 0.4551953673362732\n",
      "epoch 62 is done and the loss is 0.4410533010959625\n",
      "epoch 63 is done and the loss is 0.45510414242744446\n",
      "epoch 64 is done and the loss is 0.42720404267311096\n",
      "epoch 65 is done and the loss is 0.42564070224761963\n",
      "epoch 66 is done and the loss is 0.4044613540172577\n",
      "epoch 67 is done and the loss is 0.41242897510528564\n",
      "epoch 68 is done and the loss is 0.3697308897972107\n",
      "epoch 69 is done and the loss is 0.3609999716281891\n",
      "epoch 70 is done and the loss is 0.46102795004844666\n",
      "epoch 71 is done and the loss is 0.36103978753089905\n",
      "epoch 72 is done and the loss is 0.3953988254070282\n",
      "epoch 73 is done and the loss is 0.410528302192688\n",
      "epoch 74 is done and the loss is 0.3977303206920624\n",
      "epoch 75 is done and the loss is 0.3320944905281067\n",
      "epoch 76 is done and the loss is 0.3699382245540619\n",
      "epoch 77 is done and the loss is 0.3549041748046875\n",
      "epoch 78 is done and the loss is 0.37919604778289795\n",
      "epoch 79 is done and the loss is 0.38493335247039795\n",
      "epoch 80 is done and the loss is 0.35051676630973816\n",
      "epoch 81 is done and the loss is 0.39377927780151367\n",
      "epoch 82 is done and the loss is 0.3912409245967865\n",
      "epoch 83 is done and the loss is 0.3442342281341553\n",
      "epoch 84 is done and the loss is 0.29551154375076294\n",
      "epoch 85 is done and the loss is 0.3762211799621582\n",
      "epoch 86 is done and the loss is 0.3503530025482178\n",
      "epoch 87 is done and the loss is 0.32530879974365234\n",
      "epoch 88 is done and the loss is 0.33334657549858093\n",
      "epoch 89 is done and the loss is 0.350986510515213\n",
      "epoch 90 is done and the loss is 0.33951321244239807\n",
      "epoch 91 is done and the loss is 0.33049678802490234\n",
      "epoch 92 is done and the loss is 0.3185146152973175\n",
      "epoch 93 is done and the loss is 0.32283782958984375\n",
      "epoch 94 is done and the loss is 0.31523147225379944\n",
      "epoch 95 is done and the loss is 0.33880776166915894\n",
      "epoch 96 is done and the loss is 0.3513548970222473\n",
      "epoch 97 is done and the loss is 0.31785571575164795\n",
      "epoch 98 is done and the loss is 0.31624242663383484\n",
      "epoch 99 is done and the loss is 0.308782696723938\n",
      "epoch 100 is done and the loss is 0.2976015508174896\n",
      "epoch 101 is done and the loss is 0.2838777005672455\n",
      "epoch 102 is done and the loss is 0.27825653553009033\n",
      "epoch 103 is done and the loss is 0.31012243032455444\n",
      "epoch 104 is done and the loss is 0.2960698902606964\n",
      "epoch 105 is done and the loss is 0.30706000328063965\n",
      "epoch 106 is done and the loss is 0.3259681761264801\n",
      "epoch 107 is done and the loss is 0.3597683310508728\n",
      "epoch 108 is done and the loss is 0.2506221830844879\n",
      "epoch 109 is done and the loss is 0.3023471236228943\n",
      "epoch 110 is done and the loss is 0.31795331835746765\n",
      "epoch 111 is done and the loss is 0.32588139176368713\n",
      "epoch 112 is done and the loss is 0.32517895102500916\n",
      "epoch 113 is done and the loss is 0.30877718329429626\n",
      "epoch 114 is done and the loss is 0.2982044816017151\n",
      "epoch 115 is done and the loss is 0.26077505946159363\n",
      "epoch 116 is done and the loss is 0.26181507110595703\n",
      "epoch 117 is done and the loss is 0.354321151971817\n",
      "epoch 118 is done and the loss is 0.28830772638320923\n",
      "epoch 119 is done and the loss is 0.271513432264328\n",
      "epoch 120 is done and the loss is 0.21548983454704285\n",
      "epoch 121 is done and the loss is 0.2870872914791107\n",
      "epoch 122 is done and the loss is 0.22021038830280304\n",
      "epoch 123 is done and the loss is 0.1730288863182068\n",
      "epoch 124 is done and the loss is 0.28847959637641907\n",
      "epoch 125 is done and the loss is 0.27984318137168884\n",
      "epoch 126 is done and the loss is 0.25210025906562805\n",
      "epoch 127 is done and the loss is 0.26116257905960083\n",
      "epoch 128 is done and the loss is 0.23495744168758392\n",
      "epoch 129 is done and the loss is 0.26951003074645996\n",
      "epoch 130 is done and the loss is 0.2828446328639984\n",
      "epoch 131 is done and the loss is 0.27558019757270813\n",
      "epoch 132 is done and the loss is 0.242838054895401\n",
      "epoch 133 is done and the loss is 0.2746265232563019\n",
      "epoch 134 is done and the loss is 0.2221202552318573\n",
      "epoch 135 is done and the loss is 0.2906988263130188\n",
      "epoch 136 is done and the loss is 0.24741405248641968\n",
      "epoch 137 is done and the loss is 0.26912423968315125\n",
      "epoch 138 is done and the loss is 0.2679864466190338\n",
      "epoch 139 is done and the loss is 0.2772102952003479\n",
      "epoch 140 is done and the loss is 0.20126628875732422\n",
      "epoch 141 is done and the loss is 0.225576713681221\n",
      "epoch 142 is done and the loss is 0.16590076684951782\n",
      "epoch 143 is done and the loss is 0.32477062940597534\n",
      "epoch 144 is done and the loss is 0.21841219067573547\n",
      "epoch 145 is done and the loss is 0.22106075286865234\n",
      "epoch 146 is done and the loss is 0.22240516543388367\n",
      "epoch 147 is done and the loss is 0.2242204248905182\n",
      "epoch 148 is done and the loss is 0.2752060890197754\n",
      "epoch 149 is done and the loss is 0.21276481449604034\n",
      "epoch 150 is done and the loss is 0.19873329997062683\n",
      "epoch 151 is done and the loss is 0.20693714916706085\n",
      "epoch 152 is done and the loss is 0.17857831716537476\n",
      "epoch 153 is done and the loss is 0.20951491594314575\n",
      "epoch 154 is done and the loss is 0.1935052126646042\n",
      "epoch 155 is done and the loss is 0.16410888731479645\n",
      "epoch 156 is done and the loss is 0.1733674257993698\n",
      "epoch 157 is done and the loss is 0.1981699913740158\n",
      "epoch 158 is done and the loss is 0.20120617747306824\n",
      "epoch 159 is done and the loss is 0.16989125311374664\n",
      "epoch 160 is done and the loss is 0.15271788835525513\n",
      "epoch 161 is done and the loss is 0.1750035285949707\n",
      "epoch 162 is done and the loss is 0.14364027976989746\n",
      "epoch 163 is done and the loss is 0.17286807298660278\n",
      "epoch 164 is done and the loss is 0.1696590632200241\n",
      "epoch 165 is done and the loss is 0.1923731565475464\n",
      "epoch 166 is done and the loss is 0.23301267623901367\n",
      "epoch 167 is done and the loss is 0.18026700615882874\n",
      "epoch 168 is done and the loss is 0.20878630876541138\n",
      "epoch 169 is done and the loss is 0.18873241543769836\n",
      "epoch 170 is done and the loss is 0.14805254340171814\n",
      "epoch 171 is done and the loss is 0.16377076506614685\n",
      "epoch 172 is done and the loss is 0.16345204412937164\n",
      "epoch 173 is done and the loss is 0.14223796129226685\n",
      "epoch 174 is done and the loss is 0.16899463534355164\n",
      "epoch 175 is done and the loss is 0.1575520634651184\n",
      "epoch 176 is done and the loss is 0.19612036645412445\n",
      "epoch 177 is done and the loss is 0.1312600076198578\n",
      "epoch 178 is done and the loss is 0.15732167661190033\n",
      "epoch 179 is done and the loss is 0.18890437483787537\n",
      "epoch 180 is done and the loss is 0.11014199256896973\n",
      "epoch 181 is done and the loss is 0.1788538098335266\n",
      "epoch 182 is done and the loss is 0.1491272896528244\n",
      "epoch 183 is done and the loss is 0.14588262140750885\n",
      "epoch 184 is done and the loss is 0.157449409365654\n",
      "epoch 185 is done and the loss is 0.21095016598701477\n",
      "epoch 186 is done and the loss is 0.08862975239753723\n",
      "epoch 187 is done and the loss is 0.1699078381061554\n",
      "epoch 188 is done and the loss is 0.13504520058631897\n",
      "epoch 189 is done and the loss is 0.14583106338977814\n",
      "epoch 190 is done and the loss is 0.15479877591133118\n",
      "epoch 191 is done and the loss is 0.1042754277586937\n",
      "epoch 192 is done and the loss is 0.1562589704990387\n",
      "epoch 193 is done and the loss is 0.17791350185871124\n",
      "epoch 194 is done and the loss is 0.11347133666276932\n",
      "epoch 195 is done and the loss is 0.1672639101743698\n",
      "epoch 196 is done and the loss is 0.08775375783443451\n",
      "epoch 197 is done and the loss is 0.12920889258384705\n",
      "epoch 198 is done and the loss is 0.116291843354702\n",
      "epoch 199 is done and the loss is 0.13646677136421204\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "  for j in range(0,len(s1_train),batch_num):\n",
    "    s1_tr=torch.FloatTensor(s1_train[j:j+batch_num]).cuda()\n",
    "    s2_tr=torch.FloatTensor(s2_train[j:j+batch_num]).cuda()\n",
    "    ytrain=torch.FloatTensor(train_labels[j:j+batch_num]).cuda()\n",
    "    ff_out=model(s1_tr,s2_tr)\n",
    "    #print(ff_out.shape)\n",
    "    losses=loss(ff_out,ytrain)\n",
    "    optimize.zero_grad()\n",
    "    losses.backward()\n",
    "    optimize.step()\n",
    "  print('epoch {} is done and the loss is {}'.format(i, losses.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMi-4-dqxx7l",
    "outputId": "7aed677e-334c-4353-b670-8ba91082ceb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy is:66.5 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  s1_te=torch.FloatTensor(s1_test).cuda()\n",
    "  s2_te=torch.FloatTensor(s2_test).cuda()\n",
    "  out = model(s1_te, s2_te) \n",
    "thres = 0.5\n",
    "predict = np.zeros((s1_test.shape[0]))\n",
    "predict[out.detach().cpu().numpy() > thres] = 1\n",
    "print('Test Accuracy is:{} %'.format((sum(predict == test_labels)/s1_test.shape[0])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "OgW3l3z4YX7J"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "HW4 Part 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
